# SCST (Self-Critical Sequence Training) è®­ç»ƒä»£ç æå–

## ç›®å½•ç»“æ„

```
code/
â”œâ”€â”€ README.md                           # æœ¬æ–‡æ¡£ï¼šSCSTå®Œæ•´ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ models/
â”‚   â””â”€â”€ blip2_t5_scst.py               # BLIP2-T5 SCSTæ ¸å¿ƒæ¨¡å‹ä»£ç 
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ cider.py                       # CIDErè¯„ä¼°å™¨
â”‚   â””â”€â”€ cider_scorer.py                # CIDErè¯„åˆ†æ ¸å¿ƒç®—æ³•
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ caption_scst_example.yaml      # SCSTè®­ç»ƒé…ç½®ç¤ºä¾‹
â”‚   â””â”€â”€ caption_ce_example.yaml        # CEé¢„è®­ç»ƒé…ç½®ç¤ºä¾‹ï¼ˆSCSTå‰ç½®æ­¥éª¤ï¼‰
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train_scst.py                  # è®­ç»ƒå…¥å£è„šæœ¬
â””â”€â”€ docs/
    â””â”€â”€ SCST_GUIDE.md                  # SCSTè¯¦ç»†åŸç†ä¸å®ç°æŒ‡å—
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

SCSTï¼ˆSelf-Critical Sequence Trainingï¼‰æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œ**å¿…é¡»åœ¨ CEï¼ˆäº¤å‰ç†µï¼‰é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œ**ã€‚

#### æ­¥éª¤ 1ï¼šä½¿ç”¨ CE è®­ç»ƒåŸºç¡€æ¨¡å‹

```bash
python -m torch.distributed.run --nproc_per_node=8 train.py \
    --cfg-path configs/caption_ce_example.yaml
```

#### æ­¥éª¤ 2ï¼šä½¿ç”¨ SCST å¾®è°ƒ

```bash
python -m torch.distributed.run --nproc_per_node=8 train.py \
    --cfg-path configs/caption_scst_example.yaml
```

---

## ğŸ“– SCST æ ¸å¿ƒåŸç†

### ä»€ä¹ˆæ˜¯ SCSTï¼Ÿ

SCST æ¥è‡ªè®ºæ–‡ ["Self-critical Sequence Training for Image Captioning"](https://arxiv.org/abs/1612.00563)ã€‚

ä¼ ç»Ÿ CE è®­ç»ƒä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶ï¼ˆTeacher Forcingï¼‰ï¼Œç›´æ¥ä¼˜åŒ–é¢„æµ‹ token ä¸ ground truth çš„äº¤å‰ç†µæŸå¤±ã€‚ä½†è¿™ä¼šå¯¼è‡´ï¼š

- **Exposure Bias**ï¼šè®­ç»ƒæ—¶æ¨¡å‹çœ‹åˆ°çš„æ˜¯ ground truthï¼Œæ¨ç†æ—¶çœ‹åˆ°çš„æ˜¯è‡ªå·±çš„é¢„æµ‹
- **è¯„ä¼°æŒ‡æ ‡ä¸ä¸€è‡´**ï¼šè®­ç»ƒä¼˜åŒ– CE lossï¼Œä½†è¯„ä¼°ç”¨ CIDEr/BLEU ç­‰æŒ‡æ ‡

SCST é€šè¿‡å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ– CIDEr ç­‰è¯„ä¼°æŒ‡æ ‡ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚

### SCST æŸå¤±å‡½æ•°

```python
# æ ¸å¿ƒå…¬å¼
loss = - log_prob(sampled_caption) * (reward - baseline)

# å…¶ä¸­ï¼š
# - sampled_caption: é‡‡æ ·ç”Ÿæˆçš„caption
# - reward: é‡‡æ ·captionçš„CIDEråˆ†æ•°
# - baseline: è´ªå©ªè§£ç captionçš„CIDEråˆ†æ•°ï¼ˆæˆ–beam searchç»“æœçš„å¹³å‡åˆ†æ•°ï¼‰
```

### ä¸ºä»€ä¹ˆéœ€è¦ CE é¢„è®­ç»ƒï¼Ÿ

1. **SCST éœ€è¦åˆç†çš„åˆå§‹ç­–ç•¥**ï¼šå¦‚æœæ¨¡å‹ç”Ÿæˆçš„ caption å®Œå…¨æ˜¯åƒåœ¾ï¼Œreward éƒ½æ¥è¿‘ 0ï¼Œæ¢¯åº¦æ— æ³•æœ‰æ•ˆä¼ æ’­
2. **ç¨³å®šæ€§**ï¼šCE é¢„è®­ç»ƒåï¼Œæ¨¡å‹å·²ç»èƒ½ç”Ÿæˆåˆç†çš„ captionï¼ŒSCST åªéœ€å¾®è°ƒæå‡ CIDEr åˆ†æ•°
3. **æ”¶æ•›é€Ÿåº¦**ï¼šä»å¤´ç”¨ RL è®­ç»ƒéœ€è¦å¤§é‡æ ·æœ¬ï¼Œé¢„è®­ç»ƒååªéœ€å°‘é‡æ­¥éª¤å³å¯æ”¶æ•›

---

## ğŸ’¡ å…³é”®å®ç°ç»†èŠ‚

### 1. æ¨¡å‹ forward ä¸­çš„ SCST åˆ†æ”¯

```python
def forward(self, samples):
    if not self.scst:  # æ ‡å‡†CEè®­ç»ƒ
        # ... è®¡ç®—äº¤å‰ç†µæŸå¤±
        return {"loss": ce_loss}
    else:  # SCSTè®­ç»ƒ
        # 1. ä½¿ç”¨beam searché‡‡æ ·å¤šä¸ªcaption
        outputs = self.t5_model.generate(
            inputs_embeds=inputs_embeds,
            num_beams=self.beam_size,
            num_return_sequences=self.beam_size,
            return_dict_in_generate=True,
            output_scores=True,
        )

        # 2. è®¡ç®—æ¯ä¸ªç”Ÿæˆåºåˆ—çš„logæ¦‚ç‡
        transition_scores = self.t5_model.compute_transition_scores(...)
        sequences_scores = transition_scores.sum(dim=1) / output_length

        # 3. è®¡ç®—CIDEr reward
        reward = Cider().compute_score(caps_gt, caps_gen)

        # 4. è®¡ç®—SCSTæŸå¤±
        reward_baseline = torch.mean(reward, dim=-1, keepdim=True)
        loss = - sequences_scores * (reward - reward_baseline)

        return {"loss": loss.mean()}
```

### 2. Reward è®¡ç®—

ä½¿ç”¨ CIDEr ä½œä¸º reward æ˜¯æœ€å¸¸è§çš„é€‰æ‹©ï¼Œå› ä¸ºï¼š

- CIDEr ä¸äººç±»è¯„ä»·ç›¸å…³æ€§é«˜
- CIDEr å¯å¾®åˆ†ï¼ˆé€šè¿‡ REINFORCE æ¢¯åº¦ä¼°è®¡ï¼‰
- CIDEr å¯¹é•¿åº¦æœ‰æƒ©ç½šï¼Œé¿å…ç”Ÿæˆè¿‡é•¿æˆ–è¿‡çŸ­çš„ caption

### 3. Baseline ç­–ç•¥

æœ¬ä»£ç ä½¿ç”¨ **self-critical baseline**ï¼šä½¿ç”¨åŒä¸€ batch å†… beam search ç»“æœçš„å¹³å‡ CIDEr åˆ†æ•°ä½œä¸º baselineã€‚

```python
reward_baseline = torch.mean(reward, -1, keepdim=True)
loss = - sequences_scores * (reward - reward_baseline)
```

è¿™æ ·åšçš„å¥½å¤„ï¼š

- å‡å°æ–¹å·®
- ä¸éœ€è¦é¢å¤–çš„ baseline ç½‘ç»œ
- åŒä¸€ä¸ª batch å†…çš„æ ·æœ¬ä½œä¸ºå½¼æ­¤çš„å¯¹æ¯”

---

## âš™ï¸ è¶…å‚æ•°è°ƒä¼˜å»ºè®®

### SCST è®­ç»ƒå…³é”®è¶…å‚æ•°

| å‚æ•°           | å»ºè®®å€¼      | è¯´æ˜                                |
| -------------- | ----------- | ----------------------------------- |
| `init_lr`      | 1e-5 ~ 1e-6 | SCST å­¦ä¹ ç‡åº”æ¯” CE ä½ 1-2 ä¸ªæ•°é‡çº§  |
| `batch_size`   | 2-4         | SCST å†…å­˜å ç”¨å¤§ï¼ˆéœ€è¦é‡‡æ ·å¤šä¸ªåºåˆ—ï¼‰ |
| `beam_size`    | 5           | beam æ•°é‡ï¼Œä¹Ÿæ˜¯æ¯ä¸ªæ ·æœ¬é‡‡æ ·çš„åºåˆ—æ•° |
| `max_iters`    | 15000-20000 | æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´                  |
| `warmup_steps` | 1000        | é¢„çƒ­æ­¥æ•°                            |

### å¸¸è§é—®é¢˜

#### Q: è®­ç»ƒæ—¶ CIDEr åˆ†æ•°å‰§çƒˆæ³¢åŠ¨ï¼Ÿ

A: é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ  warmup æ­¥æ•°

#### Q: è®­ç»ƒå CIDEr åè€Œä¸‹é™ï¼Ÿ

A: æ£€æŸ¥ CE é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦è¶³å¤Ÿå¥½ï¼ˆå»ºè®® CE æ¨¡å‹ CIDEr > 60 å†è¿›è¡Œ SCSTï¼‰

#### Q: æ˜¾å­˜ä¸è¶³ï¼Ÿ

A: å‡å° batch_sizeï¼Œå‡å° beam_sizeï¼Œä½¿ç”¨ gradient accumulation

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### models/blip2_t5_scst.py

æ ¸å¿ƒæ¨¡å‹ä»£ç ï¼ŒåŒ…å«ï¼š

- CE è®­ç»ƒåˆ†æ”¯
- SCST è®­ç»ƒåˆ†æ”¯
- CIDEr reward è®¡ç®—
- åºåˆ— log æ¦‚ç‡è®¡ç®—

### evaluation/cider.py & cider_scorer.py

CIDEr è¯„ä¼°å™¨å®ç°ï¼Œç”¨äºï¼š

- è®­ç»ƒæ—¶è®¡ç®— reward
- éªŒè¯/æµ‹è¯•æ—¶è¯„ä¼°æ¨¡å‹

### configs/\*.yaml

è®­ç»ƒé…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š

- `caption_ce_example.yaml`: CE é¢„è®­ç»ƒé…ç½®
- `caption_scst_example.yaml`: SCST å¾®è°ƒé…ç½®

---

## ğŸ”§ å¦‚ä½•å°† SCST åº”ç”¨åˆ°ä½ è‡ªå·±çš„ LLM é¡¹ç›®

è¯¦ç»†æŒ‡å—è¯·å‚é˜… [docs/SCST_GUIDE.md](docs/SCST_GUIDE.md)

ç®€è¦æ­¥éª¤ï¼š

1. ç¡®ä¿ä½ çš„ LLM æ”¯æŒ `generate()` æ–¹æ³•ä¸”èƒ½è¿”å› log æ¦‚ç‡
2. å®ç° CIDEr è®¡ç®—ï¼ˆæˆ–å…¶ä»– reward å‡½æ•°ï¼‰
3. åœ¨ forward ä¸­æ·»åŠ  SCST åˆ†æ”¯
4. å…ˆç”¨ CE è®­ç»ƒå‡ºåŸºç¡€æ¨¡å‹
5. åŠ è½½ CE æ¨¡å‹ï¼Œåˆ‡æ¢åˆ° SCST æ¨¡å¼ç»§ç»­è®­ç»ƒ

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. [Self-critical Sequence Training for Image Captioning](https://arxiv.org/abs/1612.00563)
2. [BLIP-2: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2301.12597)
3. [CIDEr: Consensus-based Image Description Evaluation](https://arxiv.org/abs/1411.5726)
